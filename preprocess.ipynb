{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "+ Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import my_util as my_util; from my_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOME_DIR = 'd:/larc_projects/job_analytics/'\n",
    "DATA_DIR = HOME_DIR + 'data/clean/'\n",
    "\n",
    "# job descriptions (JDs)\n",
    "init_posts = pd.read_csv(DATA_DIR + 'jd_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skill_df = pd.read_csv(DATA_DIR + 'skill_index.csv')\n",
    "init_skills = skill_df['skill']\n",
    "jd_docs = list(init_posts['clean_text'].apply(str.lower))\n",
    "\n",
    "n_skill, n_jd = len(init_skills) , init_posts.shape[0]\n",
    "print('Initial no. of skills: %d' %n_skill)\n",
    "print('Initial no. of JDs: %d' %n_jd) # some garbage JDs with no text already removed\n",
    "\n",
    "skill_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "There are two goals: i) to remove JDs with too few skills, and ii) to remove skills occurring in too few JDs. Thus, we repeat the following process until the two goals are satisfied.\n",
    "+ Count no. of __unique__ skills in each JD\n",
    "+ Remove JDs with $<= 1$ skills\n",
    "+ Count no. of JDs containing each skill\n",
    "+ Remove skills occuring in $<= 1$ JDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter, posts = 0, init_posts\n",
    "n_post = posts.shape[0]\n",
    "\n",
    "stop_cond, thres = False, .98\n",
    "while not stop_cond:\n",
    "    n_iter = n_iter + 1\n",
    "    print('Iteration %d' %n_iter)\n",
    "    new_posts = extractJDs(posts, skills, min_n_skill=2)\n",
    "    n_new_post = new_posts.shape[0]\n",
    "    print('No. of posts after filtering: %d' %n_new_post)\n",
    "    \n",
    "    skill_df = extractSkills(skills, new_posts, min_n_jd=2)\n",
    "    new_skills = skill_df['skill']\n",
    "    print('No. of skills after filtering: %d' %len(new_skills) )\n",
    "    stop_cond = (n_new_post >= thres*n_post) and (len(new_skills) >= thres*len(skills))\n",
    "    \n",
    "    posts = new_posts\n",
    "    n_post = posts.shape[0]\n",
    "    skills = new_skills\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Save the hard-earned JDs and skills after all these filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print min(posts['n_uniq_skill'])\n",
    "# print min(skill_df['n_jd_with_skill'])\n",
    "posts.to_csv(DATA_DIR + 'filtered/posts.csv', index=False)\n",
    "skill_df.to_csv(DATA_DIR + 'filtered/skills.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Sample job postings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = posts.sort_values(by='n_uniq_skill', ascending=False)\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check by pull up skills occuring in the JD with most skills\n",
    "# post_with_most_skill = init_posts.query('job_id == {}'.format('JOB-2015-0196805') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_idx, test_idx = mkPartition(n_instance, p=80)\n",
    "X_train, X_test = doc_skill_tfidf[train_idx, :], doc_skill_tfidf[test_idx, :]\n",
    "n_train, n_test = X_train.shape[0], X_test.shape[0]\n",
    "print('Train set has %d JDs and test set has %d JDs' %(n_train, n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats = pd.DataFrame({'n_train': n_train, 'n_test': n_test, 'n_jd (train & test)': n_post, 'n_skill': len(skills)}, index=[0])\n",
    "stats.to_csv(RES_DIR + 'stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find stopword-like skills by TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ja_helpers import toIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = toIDF(terms=skills, doc_term_mat=doc_skill)\n",
    "idf.sort_values('idf_log10', inplace=True)\n",
    "idf.to_csv(SKILL_DIR + 'skill_idf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf['idf_log10'] = idf['idf'] * np.log10(np.e)\n",
    "quantile(idf['idf_log10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf_log10 = idf['idf_log10']\n",
    "n, bins, patches = plt.hist(idf_log10, bins=np.unique(idf_log10))\n",
    "plt.xlabel('IDF of term (log-10 scale)')\n",
    "plt.ylabel('# terms')\n",
    "plt.grid(True)\n",
    "plt.savefig(SKILL_DIR + 'idf_hist.pdf')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# terms which occur in at least 10% of docs\n",
    "idf.query('idf_log10 <= 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting idf threshold as 1 did not catch stop words like _com, can_, so I increase the threshold of idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf.query('idf_log10 <= 1.35').to_csv(SKILL_DIR + 'stop_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out stopword skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(SKILL_DIR + 'stop_words.csv')\n",
    "stop_words = df['term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skill_df = skill_df[- skill_df['skill'].isin(stop_words)]\n",
    "print(skill_df.shape)\n",
    "skill_df.to_csv(SKILL_DIR + 'skill_index.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle reposted jobs\n",
    "\n",
    "There are jobs reposted several times as shown below. Thus, job ids in job_posts are not unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_posts = pd.read_csv(DATA_DIR + 'full_job_posts.csv')\n",
    "job_posts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_job_id = job_posts[['job_id', 'job_posting_date_history']].groupby('job_id')\n",
    "res = by_job_id.agg({'job_posting_date_history': lambda x:len(np.unique(x))})\n",
    "\n",
    "res = res.rename(columns={'job_posting_date_history': 'n_post_date'}).reset_index()\n",
    "res.sort_values('n_post_date', ascending=False, inplace=True)\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quantile(res['n_post_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repost_jobs = res.query('n_post_date > 1')\n",
    "print('# jobs reposted: %d' %repost_jobs.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove jobs without title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs = job_posts[['job_id', 'title', 'employer_name']].drop_duplicates()\n",
    "print('# records in jobs bf merging: %d' %jobs.shape[0])\n",
    "\n",
    "jobs = pd.merge(jobs, job_desc)\n",
    "print('# records in jobs after merging: %d' %jobs.shape[0])\n",
    "\n",
    "jobs_wo_title = job_posts[job_posts['title'].isnull()]\n",
    "n_job_wo_title = jobs_wo_title.shape[0]\n",
    "print('# job posts in WDA without title: %d' %n_job_wo_title)\n",
    "\n",
    "jobs_wo_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs.to_csv(DATA_DIR + 'jobs.csv', index=False)\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean employer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "employers = pd.read_csv(DATA_DIR + 'employers.csv')\n",
    "print employers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "employers.rename(columns={'company_registration_number_uen_ep': 'employer_id', 'organisation_name_ep': 'employer_name', \n",
    "                         'ssic_group_ep': 'industry'}, inplace=True)\n",
    "\n",
    "# Standardize employer names by uppercase (problem detected below)\n",
    "employers['employer_name'] = map(str.upper, employers['employer_name'])\n",
    "employers = employers.drop_duplicates()\n",
    "employers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handle the problem with PRIORITY CONSULTANTS (detected below)\n",
    "employers.query('employer_name == \"PRIORITY CONSULTANTS\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "employers = employers.drop(10278)\n",
    "employers.query('employer_name == \"PRIORITY CONSULTANTS\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "employers.to_csv(DATA_DIR + 'employers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge doc_index, posts and employers to get industry info\n",
    "\n",
    "__Note:__ need to maintain the index in doc_index as this index is required to retrive the correct topic distribution for each document from the matrix doc_topic_distr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = pd.read_csv(DATA_DIR + 'full_job_posts.csv')\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = mergeKeepLeftIndex(doc_index, posts[['job_id', 'employer_id']])\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = mergeKeepLeftIndex(df, employers[['employer_id', 'employer_name', 'industry']])\n",
    "df = df.drop_duplicates()\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(SKILL_DIR + 'doc_index.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weird duplications in result of the first merge\n",
    "\n",
    "The duplications were then detected as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, verify duplication exists\n",
    "print len(df.index)\n",
    "print len(df.index.unique())\n",
    "\n",
    "# Then detect them\n",
    "import collections\n",
    "print [(item, count) for item, count in collections.Counter(df.index).items() if count > 1]\n",
    "\n",
    "df.iloc[25569:25571, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is due to __upper vs. lower case__ in employer names! That's why we need to standardize them.\n",
    "\n",
    "Lesson learnt: Watch out for __case-sensitive problem__ in data.\n",
    "\n",
    "After handling this, we repeat the above process and check for duplications again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print [(item, count) for item, count in collections.Counter(tmp.index).items() if count > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp.iloc[29403:29405, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it is because the company PRIORITY CONSULTANTS registered itself with __2 different industries__: Administrative Service and Scientific Activities."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
