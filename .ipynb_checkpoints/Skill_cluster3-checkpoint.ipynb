{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "+ Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.feature_extraction.text as text_manip\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time\n",
    "from scipy.sparse import *\n",
    "from my_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOME_DIR = 'd:/larc_projects/job_analytics/'\n",
    "DATA_DIR = HOME_DIR + 'data/clean/'\n",
    "REPORT_DIR = HOME_DIR + 'reports/skill_cluster/'\n",
    "\n",
    "# job descriptions (JDs)\n",
    "init_posts = pd.read_csv(DATA_DIR + 'jd_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial no. of skills: 44919\n",
      "Initial no. of JDs: 263411\n"
     ]
    }
   ],
   "source": [
    "init_skills = skill_df['skill']\n",
    "jd_docs = list(init_posts['clean_text'].apply(str.lower))\n",
    "\n",
    "n_skill, n_jd = len(init_skills) , init_posts.shape[0]\n",
    "print('Initial no. of skills: %d' %n_skill)\n",
    "print('Initial no. of JDs: %d' %n_jd) # some garbage JDs with no text already removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of unigram, bigram and trigram skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skill</th>\n",
       "      <th>n_word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>1</td>\n",
       "      <td>170302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>management</td>\n",
       "      <td>1</td>\n",
       "      <td>161636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>156517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        skill  n_word    freq\n",
       "0    business       1  170302\n",
       "1  management       1  161636\n",
       "2     support       1  156517"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skill_df = pd.read_csv(REPORT_DIR + 'skill_stats.csv')\n",
    "skill_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_bigram_skill</th>\n",
       "      <th>n_trigram_skill</th>\n",
       "      <th>n_unigram_skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20386</td>\n",
       "      <td>10778</td>\n",
       "      <td>7537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_bigram_skill  n_trigram_skill  n_unigram_skill\n",
       "0           20386            10778             7537"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_gram_skills = list(skill_df.query('n_word == 1')['skill'])\n",
    "bi_gram_skills = list(skill_df.query('n_word == 2')['skill'])\n",
    "tri_gram_skills = list(skill_df.query('n_word == 3')['skill'])\n",
    "\n",
    "pd.DataFrame({'n_unigram_skill': len(uni_gram_skills), 'n_bigram_skill': len(bi_gram_skills), \n",
    "              'n_trigram_skill': len(tri_gram_skills)}, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14829"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills = pd.Series.unique(skill_df.query('freq > 0')['skill'])\n",
    "len(skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No. of unique uni-grams per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "print('Counting occurrence of uni-gram skills...')\n",
    "uni_gram_vectorizer = text_manip.CountVectorizer(vocabulary=skills)  \n",
    "doc_unigram_freq = uni_gram_vectorizer.fit_transform(jd_docs)\n",
    "print('Done after %.1fs' %(time() - t0))\n",
    "\n",
    "## For each doc, \"its no. of unique uni-grams = no. of non-zero counts\" in its row in doc-term mat\n",
    "def n_non_zero(r, sp_mat):\n",
    "    return len(sp_mat.getrow(r).nonzero()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50% (median)</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min  25%  50% (median)   75%    max\n",
       "0  0.0  8.0          14.0  22.0  119.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary_vectorizer = text_manip.CountVectorizer(vocabulary=skills, binary=True)\n",
    "# print('Marking unique unigram skills in JDs...')\n",
    "# t0 = time()\n",
    "# doc_unigram_occurrence = binary_vectorizer.fit_transform(jd_docs)\n",
    "# print('Done after %.1fs' %(time() - t0))\n",
    "# init_posts['n_uniq_unigram'] = doc_unigram_occurrence.sum(axis=1).A1\n",
    "quantile(init_posts['n_uniq_unigram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(n_uniq_unigram, bins=np.unique(init_posts['n_uniq_unigram']))\n",
    "plt.xlabel('no. of unique unigrams in JD')\n",
    "plt.ylabel('no. of JDs')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No. of unique skills per JDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here each skill can be a uni-, bi-, or tri-gram (i.e. len(skill) <= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove skills never occurring in JDs\n",
    "This step is already done in previous run, no need to do again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we work with the set of __occurring__ skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count no. of unique skills in each JD by binary vectorizer\n",
    "binary_vectorizer = text_manip.CountVectorizer(vocabulary=occur_skills, ngram_range=(1, max_n_word), binary=True)\n",
    "t0 = time()\n",
    "print('Marking occurrence of skills with length <= %d ...' %max_n_word)\n",
    "doc_skill_occurrence = binary_vectorizer.fit_transform(jd_docs)\n",
    "print('Done after %.1fs' %(time() - t0))\n",
    "\n",
    "init_posts['n_uniq_skill'] = doc_skill_occurrence.sum(axis=1).A1\n",
    "quantile(init_posts['n_uniq_skill'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "There are two goals: i) to remove JDs with too few skills, and ii) to remove skills occurring in too few JDs. Thus, we repeat the following process until the two goals are satisfied.\n",
    "+ Count no. of __unique__ skills in each JD\n",
    "+ Remove JDs with $<= 1$ skills\n",
    "+ Count no. of JDs containing each skill\n",
    "+ Remove skills occuring in $<= 1$ JDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter, posts = 0, init_posts\n",
    "n_post = posts.shape[0]\n",
    "\n",
    "stop_cond, thres = False, .98\n",
    "while not stop_cond:\n",
    "    n_iter = n_iter + 1\n",
    "    print('Iteration %d' %n_iter)\n",
    "    new_posts = extractJDs(posts, skills, min_n_skill=2)\n",
    "    n_new_post = new_posts.shape[0]\n",
    "    print('No. of posts after filtering: %d' %n_new_post)\n",
    "    \n",
    "    skill_df = extractSkills(skills, new_posts, min_n_jd=2)\n",
    "    new_skills = skill_df['skill']\n",
    "    print('No. of skills after filtering: %d' %len(new_skills) )\n",
    "    stop_cond = (n_new_post >= thres*n_post) and (len(new_skills) >= thres*len(skills))\n",
    "    \n",
    "    posts = new_posts\n",
    "    n_post = posts.shape[0]\n",
    "    skills = new_skills\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Save the hard-earned JDs and skills after all these filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print min(posts['n_uniq_skill'])\n",
    "# print min(skill_df['n_jd_with_skill'])\n",
    "posts.to_csv(DATA_DIR + 'filtered/posts.csv', index=False)\n",
    "skill_df.to_csv(DATA_DIR + 'filtered/skills.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Sample job postings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts = posts.sort_values(by='n_uniq_skill', ascending=False)\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check by pull up skills occuring in the JD with most skills\n",
    "# post_with_most_skill = init_posts.query('job_id == {}'.format('JOB-2015-0196805') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_idx, test_idx = mkPartition(n_instance, p=80)\n",
    "X_train, X_test = doc_skill_tfidf[train_idx, :], doc_skill_tfidf[test_idx, :]\n",
    "n_train, n_test = X_train.shape[0], X_test.shape[0]\n",
    "print('Train set has %d JDs and test set has %d JDs' %(n_train, n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats = pd.DataFrame({'n_train': n_train, 'n_test': n_test, 'n_jd (train & test)': n_post, 'n_skill': len(skills)}, index=[0])\n",
    "stats.to_csv(RES_DIR + 'stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set global arguments:\n",
    "   + no. of topics: _k_ in {5, 10, ..., 20}\n",
    "   + no. of top words to be printed out in result\n",
    "   + directory to save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RES_DIR = REPORT_DIR + 'r6/'\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ks  = range(5, 25, 5)\n",
    "ks = range(4, 24, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill Clustering by NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf_vect = text_manip.TfidfVectorizer(vocabulary=skills, ngram_range=(1, max_n_word))\n",
    "n_instance, n_feat = posts.shape[0], len(skills)\n",
    "t0 =time()\n",
    "print('Building tf_idf for %d JDs using %d features (skills)...' %(n_instance, n_feat))\n",
    "doc_skill_tfidf = tf_idf_vect.fit_transform(posts['clean_text'])\n",
    "print('Done after %.1fs' %(time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnmf = {k: NMF(n_components=k, random_state=0) for k in ks}\n",
    "print( \"Fitting NMF using random initialization...\" )\n",
    "print('No. of topics, Error, Running time')\n",
    "rnmf_error = []\n",
    "\n",
    "for k in ks:\n",
    "    t0 = time()\n",
    "    rnmf[k].fit(X_train)\n",
    "    elapsed = time() - t0\n",
    "    err = rnmf[k].reconstruction_err_\n",
    "    print('%d, %0.1f, %0.1fs' %(k, err, elapsed))\n",
    "    rnmf_error.append(err)\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Save each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmf_features = tf_idf_vect.get_feature_names()\n",
    "pd.DataFrame(nmf_features).to_csv(RES_DIR + 'nmf_features.csv', index=False)\n",
    "\n",
    "for k in ks:\n",
    "    top_words = top_words_df(n_top_words, model=rnmf[k],feature_names=nmf_features)\n",
    "    top_words.to_csv(RES_DIR + 'nmf_{}_topics.csv'.format(k), index=False)\n",
    "#   each word dist is a component in NMF\n",
    "    word_dist = pd.DataFrame(rnmf[k].components_).apply(normalize, axis=1)\n",
    "    word_dist.to_csv(RES_DIR + 'nmf_word_dist_{}topics.csv'.format(k), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Calculating test errors of random NMF ...')\n",
    "rnmf_test_error = cal_test_err(mf_models=rnmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_k = ks[np.argmin(rnmf_test_error)]\n",
    "print('The best no. of topics is %d' %best_k)\n",
    "rnmf_best = rnmf[best_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmf_fig = plotMetrics(train_metric=rnmf_error, test_metric=rnmf_test_error, model_name='NMF')\n",
    "nmf_fig.savefig(RES_DIR + 'nmf.pdf')\n",
    "plt.close(nmf_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill Clustering by LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "print('Building count features for LDA from %d JDs and %d skills...' %(n_post, len(skills)))\n",
    "count_vectorizer = text_manip.CountVectorizer(vocabulary=skills, ngram_range=(1, max_n_word))\n",
    "doc_skill_freq = count_vectorizer.fit_transform(posts['clean_text'])\n",
    "print('Done after %.1fs' %(time() - t0))\n",
    "doc_skill_freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_X_train, lda_X_test = doc_skill_freq[train_idx, :], doc_skill_freq[test_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_scores = []\n",
    "lda = {k: LatentDirichletAllocation(n_topics=k, max_iter=5, learning_method='online', learning_offset=50.,\n",
    "                                   random_state=0) # verbose=1\n",
    " for k in ks}\n",
    "\n",
    "print(\"Fitting LDA ...\")\n",
    "print('No. of topics, Log-likelihood, Running time')\n",
    "\n",
    "for k in ks:\n",
    "    t0 = time()\n",
    "    lda[k].fit(lda_X_train)\n",
    "    s = lda[k].score(lda_X_train)\n",
    "    print('%d, %0.1f, %0.1fs' %(k, s, time() - t0))\n",
    "    lda_scores.append(s)\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of LDA on test set by perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perp = [lda[k].perplexity(lda_X_test) for k in ks]\n",
    "perp_df = pd.DataFrame({'No. of topics': ks, 'Perplexity': perp})\n",
    "perp_df.to_csv(RES_DIR + 'perplexity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_best_k = ks[np.argmin(perp)]\n",
    "print('Best no. of topics for LDA: %d' %lda_best_k)\n",
    "perp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Save LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_feats = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(lda_feats).to_csv(RES_DIR + 'lda_feats.csv')\n",
    "\n",
    "for k in ks:\n",
    "    word_dist = pd.DataFrame(lda[k].components_).apply(normalize, axis=1)\n",
    "    word_dist.to_csv(RES_DIR + 'lda_word_dist_{}topics.csv'.format(k), index=False)\n",
    "    \n",
    "    lda_topics = top_words_df(n_top_words, model=lda[k], feature_names=lda_feats)\n",
    "    lda_topics.to_csv(RES_DIR + 'lda_{}topics.csv'.format(k), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put all model metrics on training & test datasets into 2 data frames\n",
    "model_list = ['LDA', 'randomNMF']\n",
    "\n",
    "train_metric = pd.DataFrame({'No. of topics': ks, 'LDA': np.divide(lda_scores, 10**6), 'randomNMF': rnmf_error})\n",
    "test_metric = pd.DataFrame({'No. of topics': ks, 'LDA': perp, 'randomNMF': rnmf_test_error, })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, model in enumerate(model_list):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.subplots_adjust(wspace=.5, hspace=.5)  \n",
    "    #     train metric\n",
    "    plt.title(model)\n",
    "    plt.plot(ks, train_metric[model], '--')\n",
    "    plt.xlabel('No. of topics')\n",
    "    if model == 'LDA':\n",
    "        plt.ylabel(r'Log likelihood ($\\times 10^6$)')\n",
    "    else:\n",
    "        plt.ylabel(r'$\\| X_{train} - W_{train} H \\|_2$')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(ks)\n",
    "    \n",
    "    #     test metric\n",
    "    plt.subplot(2, 2, i+3)\n",
    "    plt.title(model)\n",
    "    plt.plot(ks, test_metric[model], 'r')\n",
    "    plt.xlabel('No. of topics')\n",
    "    if model == 'LDA':\n",
    "        plt.ylabel(r'Perplexity')\n",
    "    else:\n",
    "        plt.ylabel(r'$\\| X_{test} - W_{test} H \\|_2$')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(ks)\n",
    "        \n",
    "# end\n",
    "plt.show()\n",
    "fig.savefig(RES_DIR + 'lda_vs_nmf.pdf')\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
