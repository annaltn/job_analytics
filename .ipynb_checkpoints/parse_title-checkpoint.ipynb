{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep track of titles which cannot be parsed\n",
    "global invalid_titles; invalid_titles = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import parse_funs as parse_funs; from parse_funs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed dirs\n",
    "HOME_DIR = 'd:/larc_projects/job_analytics/'\n",
    "DATA_DIR = HOME_DIR + 'data/clean/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR + 'doc_index_filter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = list(df['title'].unique())\n",
    "n_title = len(titles)\n",
    "print('# unique titles: %d' % n_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_stats = pd.read_csv(DATA_DIR + 'stats_job_titles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing job titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to divide the parsing process into several medium-size batches as this is a good practice when we are using a web service or access web/remote server. The division also allows us to easily locate a bug when it occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseBatch(b, start=None, end=None):\n",
    "    '''\n",
    "    @brief: parse a batch of 100 titles\n",
    "    @return: DF containing results of parsing the 100 titles in batch b\n",
    "    '''\n",
    "    print('Parsing batch {}...'.format(b))\n",
    "    if (not start) and (not end): start = 100*b; end = start + 100\n",
    "    batch_titles = titles[start:end]\n",
    "    frames = [parse(t) for t in batch_titles]\n",
    "    res = pd.concat(frames)\n",
    "    res.reset_index(inplace=True); del res['index']\n",
    "    time.sleep(3)\n",
    "    \n",
    "#     defensive save result of the batch\n",
    "    fname = DATA_DIR + 'tmp/b{}.csv'.format(b)\n",
    "    res.to_csv(fname, index=False)\n",
    "    print('\\t saved result to tmp file')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_batch = n_title/100; remainder = n_title % 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [parseBatch(b) for b in range(n_batch)]\n",
    "\n",
    "rem_titles = titles[-remainder:]\n",
    "last_frame = pd.concat([parse(t) for t in rem_titles])\n",
    "\n",
    "frames.append(last_frame)\n",
    "\n",
    "res = pd.concat(frames)\n",
    "print res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Check for duplicates in results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('# dups in parsing result: %d' %sum(res.duplicated('title')))\n",
    "res[res.duplicated('title')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Save invalid titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "invalid_titles = res.title[res.duplicated('title')].unique()\n",
    "print('# invalid titles: %d' %len(invalid_titles))\n",
    "pd.DataFrame({'title': invalid_titles}).to_csv(DATA_DIR + 'invalid_titles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Rm dups due to invalid titles and replace NAs by empty strings (to avoid NAs destroying standard titles later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = res.drop_duplicates('title')\n",
    "print res.shape\n",
    "\n",
    "res.domain.fillna('', inplace=True); res.position.fillna('', inplace=True)\n",
    "res.pri_func.fillna('', inplace=True); res.sec_func.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res.to_csv(DATA_DIR + 'parsed_titles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Job Titles\n",
    "\n",
    "The standard form for all titles is: __position + domain + (secondary function) + primary function__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def camelCase(s):\n",
    "    return s.title()\n",
    "\n",
    "def joinValidParts(row):\n",
    "    '''\n",
    "    @brief: Only join valid parts with data, parts which are empty str are removed\n",
    "    '''\n",
    "    parts = [row.position, row.domain, row.sec_func, row.pri_func]\n",
    "    valid_parts = [p for p in parts if p != '']\n",
    "#     print('# valid parts: %d' %len(valid_parts))\n",
    "    return ' '.join(valid_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This naive join will cause confusing results\n",
    "res['std_title'] = res.position + ' ' + res.domain + ' ' + res.sec_func + ' ' + res.pri_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helps to detect the confusing problem: __non_std_title seems identical with title__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = res.query('non_std_title != std_title').iloc[0]\n",
    "my_util.tagChanges(row.non_std_title, row.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that the problem is due to NA parts, e.g. sec-func, which add spaces (almost invisible to naked eyes) to the std-ized version. Thus we need a better join which combines only valid parts. That made me create joinValidParts()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = res.query('non_std_title == \"Site Engineer\"').iloc[0]\n",
    "print joinValidParts(row)\n",
    "\n",
    "' '.join([row.position, row.domain, row.sec_func, row.pri_func])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res['std_title'] = res.apply(joinValidParts, axis=1)\n",
    "res.std_title = res.std_title.apply(camelCase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From now on, title means standard title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res.rename(columns={'std_title': 'title'}, inplace=True)\n",
    "res.to_csv(DATA_DIR + 'parsed_titles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stdForm = dict(zip(res.non_std_title, res.title))\n",
    "\n",
    "print('# non-std titles which can be standardized: %d' %len(stdForm.keys()))\n",
    "\n",
    "uniq_std_titles = np.unique((stdForm.values()))\n",
    "print('# unique standard titles: %d' % len(uniq_std_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing Job Titles of Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toStd(t):\n",
    "    if t not in stdForm.keys():\n",
    "        return t\n",
    "    else:\n",
    "        if stdForm[t] == '': # for titles which could not be parsed\n",
    "            return t\n",
    "        else: \n",
    "            return stdForm[t]\n",
    "            \n",
    "    #     return stdForm[t] if t in stdForm.keys() else t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR + 'doc_index_filter.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniq_non_std_titles = df['non_std_title'].unique()\n",
    "# sum([t not in stdForm.keys() for t in uniq_non_std_titles])\n",
    "tmp = [t for t in uniq_non_std_titles if t not in stdForm.keys()]\n",
    "tmp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"# unique non-std titles in data: %d\" % df.non_std_title.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['title'] = map(toStd, df['non_std_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Sanity check if the std-ation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.query('non_std_title != title')[['non_std_title', 'title']].head()\n",
    "any(df.title == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR + 'doc_index_filter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "+ Re-query job titles with at least 2 posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "by_title_agg = df.groupby('title').agg({'job_id': 'nunique'})\n",
    "by_title_agg.rename(columns={'job_id': 'n_post'}, inplace=True)\n",
    "by_title_agg.reset_index(inplace=True)\n",
    "by_title_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles2_ = by_title_agg.query('n_post >= 2')\n",
    "print('# titles with >= 2 posts: %d' %titles2_.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_df = pd.merge(titles2_, res)\n",
    "print(title_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# res.columns\n",
    "title_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_df.to_csv(DATA_DIR + 'new_titles_2posts_up.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domains in Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "by_domain = res.groupby('domain')\n",
    "print('# domains: {}'.format(by_domain.ngroups))\n",
    "\n",
    "by_domain_agg = by_domain.agg({'title': len})\n",
    "by_domain_agg = by_domain_agg.add_prefix('n_').reset_index()\n",
    "by_domain_agg.sort_values('n_title', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_domain_agg.describe().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the total no. of domains is large, we actually only interested in domains with at least $2$ job titles. The reason is the domains with only $1$ title give us no pairwise similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domains_2 = by_domain_agg.query('n_title >= 2')\n",
    "print('# domains with at least 2 job titles: %d' %len(domains_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "by_domain_agg.to_csv(DATA_DIR + 'stats_domains.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary Functions in Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "by_pri_func = res.groupby('pri_func')\n",
    "print('# primary functions: {}'.format(by_pri_func.ngroups))\n",
    "\n",
    "by_pri_func_agg = by_pri_func.agg({'title': len})\n",
    "by_pri_func_agg = by_pri_func_agg.add_prefix('n_').reset_index()\n",
    "by_pri_func_agg.sort_values('n_title', ascending=False, inplace=True)\n",
    "\n",
    "by_pri_func_agg.to_csv(DATA_DIR + 'stats_pri_funcs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_pri_func_agg.describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_pri_func_agg.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test parser web service (src by Koon Han)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# r0 = requests.post(parse_url, auth=(user, pwd), \n",
    "#                   json={\"job_title\":\"accountant\", \"verbose\": False})\n",
    "\n",
    "# print r0.status_code\n",
    "# print r0.json()['output']\n",
    "\n",
    "r1 = requests.post(parse_url, auth=(user, pwd), \n",
    "                  json={\"job_title\":\"software developer\", \"verbose\": False})\n",
    "\n",
    "print r1.status_code\n",
    "print r1.json()['output']\n",
    "\n",
    "\n",
    "# r2 = requests.post(parse_url, auth=(user, pwd), \n",
    "#                   json={\"job_title\":\"pre-school teacher\", \"verbose\": False})\n",
    "\n",
    "# print r2.status_code\n",
    "# print r2.json()['output']\n",
    "\n",
    "# r3 = requests.post(parse_url, auth=(user, pwd), \n",
    "#                   json={\"job_title\":\"Assistant Civil and Structural Engineer\", \n",
    "#                         \"verbose\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r4 = requests.post(parse_url, auth=(user, pwd), \n",
    "                  json={\"job_title\": \"Security Analyst, Information Technology\",\n",
    "                        \"verbose\": False})\n",
    "print r4.json()['output']\n",
    "\n",
    "r5 = requests.post(parse_url, auth=(user, pwd), \n",
    "                  json={\"job_title\": \"Data analyst and scientist\",\n",
    "                        \"verbose\": False})\n",
    "print r5.json()['output']"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
