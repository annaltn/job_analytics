{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Document-Skill matrix\n",
    "\n",
    "We first focus on building the document-skill matrix where each entry $f(d,s)$ is the frequency skill $s$ occurs in document $d$.\n",
    "\n",
    "__Challenge__: naive counting can cause over-estimating the frequency. This is due to _overlapping_ in skills. By _overlapping_ we mean that a unigram skill can be a part of a bigram/trigram skill. For example: 'analytics' is a skill itself but it also occurs in 'data analytics', thus a document with skill 'data analytics' occuring 10 times is also considered as containing skill 'analytics' 10 times.\n",
    "\n",
    "__Solution__: To overcome this, we propose _counting with removal_ as follows. We count trigram skills first, then remove them from docs, count bigram skills, remove them from docs and finally count unigram skills. This way we can avoid _overlapping_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cluster_skill_helpers as cluster_skill_helpers\n",
    "from cluster_skill_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOME_DIR = 'd:/larc_projects/job_analytics/'; DATA_DIR = HOME_DIR + 'data/clean/'\n",
    "SKILL_DIR = DATA_DIR + 'skill_cluster/'; RES_DIR = HOME_DIR + 'results/reports/skill_cluster/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jd_df = pd.read_csv(DATA_DIR + 'jd_df.csv')\n",
    "jd_df.sort_values(by='n_uniq_skill', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jd_docs = jd_df['clean_text'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_index = pd.DataFrame({'job_id': jd_df['job_id'], 'doc': jd_docs})\n",
    "doc_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_index.to_csv(SKILL_DIR + 'doc_index.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skill_df = pd.read_csv(DATA_DIR + 'skill_cluster/skill_df.csv')\n",
    "skills = skill_df['skill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting with removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_skills = np.unique(skill_df.query('n_word == 3')['skill'])\n",
    "bigram_skills = np.unique(skill_df.query('n_word == 2')['skill'])\n",
    "unigram_skills = np.unique(skill_df.query('n_word == 1')['skill'])\n",
    "\n",
    "# pd.DataFrame({'trigram': trigram_skills}).to_csv(SKILL_DIR + 'trigrams.csv', index=False)\n",
    "# pd.DataFrame({'bigram': bigram_skills}).to_csv(SKILL_DIR + 'bigrams.csv', index=False)\n",
    "# pd.DataFrame({'unigram': unigram_skills}).to_csv(SKILL_DIR + 'unigrams.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(cluster_skill_helpers)\n",
    "from cluster_skill_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Trigram skills:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_trigram = buildDocSkillMat(n=3, jd_docs=jd_docs, skills=trigram_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing tri-grams from JDs to avoid duplications...\n",
      "Removal for 10000 docs and counting...\n",
      "Removal for 20000 docs and counting...\n",
      "Removal for 30000 docs and counting...\n",
      "Removal for 40000 docs and counting...\n",
      "Removal for 50000 docs and counting...\n",
      "Removal for 60000 docs and counting...\n",
      "Removal for 70000 docs and counting...\n",
      "Removal for 80000 docs and counting...\n",
      "Removal for 90000 docs and counting...\n",
      "Removal for 100000 docs and counting...\n",
      "Removal for 110000 docs and counting...\n",
      "Removal for 120000 docs and counting...\n",
      "Removal for 130000 docs and counting...\n",
      "Removal for 140000 docs and counting...\n",
      "Removal for 150000 docs and counting...\n",
      "Removal for 160000 docs and counting...\n",
      "Removal for 170000 docs and counting...\n",
      "Removal for 180000 docs and counting...\n",
      "Removal for 190000 docs and counting...\n",
      "Removal for 200000 docs and counting...\n",
      "Removal for 210000 docs and counting...\n",
      "Removal for 220000 docs and counting...\n",
      "Removal for 230000 docs and counting...\n",
      "Removal for 240000 docs and counting...\n",
      "Removal for 250000 docs and counting...\n",
      "Removal for 260000 docs and counting...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Removing tri-grams from JDs to avoid duplications...')\n",
    "jd_docs = jd_docs.apply(rmSkills, skills = trigram_skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Bigram skills:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(cluster_skill_helpers)\n",
    "from cluster_skill_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting occurrence of 2-gram skills...\n",
      "Done after 41.1s\n",
      "Removing bi-grams from JDs...\n",
      "Removal for 10000 docs and counting...\n",
      "Removal for 20000 docs and counting...\n",
      "Removal for 30000 docs and counting...\n",
      "Removal for 40000 docs and counting...\n",
      "Removal for 50000 docs and counting...\n",
      "Removal for 60000 docs and counting...\n",
      "Removal for 70000 docs and counting...\n",
      "Removal for 80000 docs and counting...\n",
      "Removal for 90000 docs and counting...\n",
      "Removal for 100000 docs and counting...\n",
      "Removal for 110000 docs and counting...\n",
      "Removal for 120000 docs and counting...\n",
      "Removal for 130000 docs and counting...\n",
      "Removal for 140000 docs and counting...\n",
      "Removal for 150000 docs and counting...\n",
      "Removal for 160000 docs and counting...\n",
      "Removal for 170000 docs and counting...\n",
      "Removal for 180000 docs and counting...\n",
      "Removal for 190000 docs and counting...\n",
      "Removal for 200000 docs and counting...\n",
      "Removal for 210000 docs and counting...\n",
      "Removal for 220000 docs and counting...\n",
      "Removal for 230000 docs and counting...\n",
      "Removal for 240000 docs and counting...\n",
      "Removal for 250000 docs and counting...\n",
      "Removal for 260000 docs and counting...\n"
     ]
    }
   ],
   "source": [
    "doc_bigram = buildDocSkillMat(n=2, jd_docs=jd_docs, skills=bigram_skills)\n",
    "print('Removing bi-grams from JDs...')\n",
    "jd_docs = jd_docs.apply(rmSkills, skills = bigram_skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Unigram skills:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting occurrence of 1-gram skills...\n",
      "Done after 28.1s\n"
     ]
    }
   ],
   "source": [
    "doc_unigram = buildDocSkillMat(n=1, jd_docs=jd_docs, skills=unigram_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with(open(SKILL_DIR + 'doc_trigram.mtx', 'w')) as f:\n",
    "    mmwrite(f, doc_trigram)\n",
    "with(open(SKILL_DIR + 'doc_bigram.mtx', 'w')) as f:\n",
    "    mmwrite(f, doc_bigram)    \n",
    "with(open(SKILL_DIR + 'doc_unigram.mtx', 'w')) as f:\n",
    "    mmwrite(f, doc_unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Concat matrices doc_unigram, doc_bigram and doc_trigram to get occurrences of all skills:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "doc_skill = hstack([doc_unigram, doc_bigram, doc_trigram])\n",
    "assert doc_skill.shape[0] == doc_unigram.shape[0]\n",
    "assert doc_skill.shape[1] == doc_unigram.shape[1] + doc_bigram.shape[1] + doc_trigram.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with(open(SKILL_DIR + 'doc_skill.mtx', 'w')) as f:\n",
    "    mmwrite(f, doc_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skills = np.concatenate((unigram_skills, bigram_skills, trigram_skills))\n",
    "pd.DataFrame({'skill': skills}).to_csv(SKILL_DIR + 'skill_index.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = text_manip.CountVectorizer(vocabulary=skills, ngram_range=(1,3))\n",
    "\n",
    "t0 = time()\n",
    "print('Naive counting...')\n",
    "naive_doc_skill = vectorizer.fit_transform(jd_docs)\n",
    "print('Done after %f.1s' %(time() - t0))\n",
    "\n",
    "s_freq = naive_doc_skill.sum(axis=0).A1\n",
    "naive_skill_df = pd.DataFrame({'skill': skills, 'freq': s_freq})\n",
    "naive_skill_df = pd.merge(naive_skill_df, skill_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "naive_skill_df = naive_skill_df[['skill', 'n_word', 'freq', 'n_jd_with_skill']]\n",
    "naive_skill_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = vectorizer.inverse_transform(naive_doc_skill)\n",
    "# res[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most popular skills after the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_freq = doc_skill.sum(axis=0).A1\n",
    "new_skill_df = pd.DataFrame({'skill': skills, 'new_freq': s_freq})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skill_df = pd.merge(naive_skill_df, new_skill_df)\n",
    "skill_df = skill_df[['skill', 'n_word', 'freq', 'new_freq', 'n_jd_with_skill']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_df = skill_df.query('n_word == 1').sort_values(by='new_freq', ascending=False)\n",
    "bigram_df = skill_df.query('n_word == 2').sort_values(by='new_freq', ascending=False)\n",
    "trigram_df = skill_df.query('n_word == 3').sort_values(by='new_freq', ascending=False)\n",
    "\n",
    "print('# unigram skills in JDs: {}'.format(unigram_df.shape[0]))\n",
    "print('# bigram skills in JDs: {}'.format(bigram_df.shape[0]))\n",
    "print('# trigram skills in JDs: {}'.format(trigram_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Top-10 popular __trigram__ skills:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Top-k popular __bigram__ skills:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Top-k popular __unigram__ skills:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trigram_df.to_csv(SKILL_DIR + 'trigram.csv', index=False)\n",
    "bigram_df.to_csv(SKILL_DIR + 'bigram.csv', index=False)\n",
    "unigram_df.to_csv(SKILL_DIR + 'unigram.csv', index=False)\n",
    "\n",
    "# top100_skills = skill_df.head(100)\n",
    "# top100_skills.to_csv(RES_DIR + 'top100_skills.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA and NMF on New Doc-Skill Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global arguments:\n",
    "   + no. of topics: _k_ in {5, 10, ..., 20}\n",
    "   + no. of top words to be printed out in result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ks  = range(5, 25, 5)\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_doc = doc_skill.shape[0]; n_feat = doc_skill.shape[1]\n",
    "train_idx, test_idx = mkPartition(n_doc, p=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skill Clustering by LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_X_train, lda_X_test = doc_skill[train_idx, :], doc_skill[test_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # check correctness of rmSkills()\n",
    "# non_zeros = find(doc_trigram)\n",
    "# idx_docs_with_trigram = non_zeros[0]\n",
    "# trigram_counts = non_zeros[2]\n",
    "# many_trigrams = idx_docs_with_trigram[trigram_counts > 1]\n",
    "\n",
    "# doc_with_trigram = jd_docs.iloc[many_trigrams[0]]\n",
    "# print('Doc bf removing tri-gram skills:\\n {}'.format(doc_with_trigram))\n",
    "# res = rmSkills(trigram_skills, doc_with_trigram)\n",
    "\n",
    "# two_spaces = [m.start() for m in re.finditer('  ', res)]\n",
    "# print res[two_spaces[1]:]\n",
    "# print res[two_spaces[0]:450]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
